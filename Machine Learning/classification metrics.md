While for regression problems the RMSE or MSE are good metrics to evaluate a model's performance, for classification problems it is a bit more complicated. When we only look at accuracy, we can be mislead because of the distribution of classification labels in the data. For example, a model that always predict the label 0 in binary classification has 90% accuracy if the distribution of labels in the data is 90/10 (0/1).
So, we used other metrics such as sensitivity/recall, specificity, precision, f1-score. The ROC curve and ROC AUC score are other metrics frequently used.

The confusion matrix contains most of the relevant information:
![[confusion_matrix.png]]

Precision : 'From the detected cats, how many were actually cats'.
Recall : 'Correctly detected cats over total cats'.

Recall (True positive rate), specificity (True negative rate) and precision (Positive predictive value) are expressed:
$$\text{Recall} = \frac{TP}{TP + FN}$$
$$\text{Specificity} = \frac{TN}{FP + TN}$$
$$\text{Precision} = \frac{TP}{TP + FP}$$
The F1-score is the harmonic mean of precision and recall:
$$\text{F1} = \frac{2TP}{2TP + FP + FN}$$
For classification problems, there is a precision-recall tradeoff. The F1 score prioritizes high and close values for precision and recall. If there is an imbalance, it is lower. This tradeoff can be shown with the precision vs recall curve:
![[precision_recall.png]]
Precision and recall versus the decision threshold.

Accuracy can also be computed:
$$\text{Accuracy} = \frac{TP+ TN}{TP + FN + FP +TN}$$

The ROC (Receiver-Operator Characteristic) is the plot of TPR (recall) vs FPR (1 - specificity) for all possible thresholds (generated by the classifier). We then compute the Area Under the Curve (AUC). The larger the AUC, the better the overall performance.

![[roc.png]]
ROC Curve.